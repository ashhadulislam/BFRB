{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd8699-a2c3-4da9-a081-73b206852857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Behavior with Sensor Data – CNN + Bi-LSTM + Demographics\n",
    "# ------------------------------------------------------------------\n",
    "# This is a minimally-intrusive revision of your original notebook.\n",
    "# The only functional addition is that the seven demographic/anthro-\n",
    "# pometric columns from train_demographics.csv are merged onto every\n",
    "# row of the sensor frame and treated as extra numeric channels.\n",
    "# Nothing else in the pipeline changes, so you can reuse previous\n",
    "# hyper-parameters and checkpoints if desired.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "#from tensorflow.keras.models import Sequential, load_model\n",
    "#from tensorflow.keras.layers import (\n",
    "#    Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization,\n",
    "#    LSTM, Bidirectional, GlobalAveragePooling1D\n",
    "#)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "#import tensorflow as tf\n",
    "import polars as pl\n",
    "#import kaggle_evaluation.cmi_inference_server  # noqa: F401   | Kaggle runner hook\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sktime.transformations.panel.rocket import Rocket, MiniRocket\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# custom model\n",
    "from detach_rocket.detach_classes import DetachEnsemble\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65b590-f501-4f55-ac08-d3f62abfcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper: compute metrics safely\n",
    "# -----------------------------\n",
    "def compute_binary_metrics(y_true, y_pred, y_proba=None):\n",
    "    # Confusion matrix with fixed label order to always get TN,FP,FN,TP\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    print(cm)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Positive-class metrics (pos_label=1); zero_division=0 to avoid NaNs\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Specificity = TN / (TN + FP); guard divide-by-zero\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    # Probability-based metrics (skip if proba missing or only one class present)\n",
    "    auroc = None\n",
    "    auprc = None\n",
    "    brier = None\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true, y_proba)\n",
    "        except Exception:\n",
    "            auroc = None\n",
    "        try:\n",
    "            auprc = average_precision_score(y_true, y_proba)\n",
    "        except Exception:\n",
    "            auprc = None\n",
    "        try:\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except Exception:\n",
    "            brier = None\n",
    "\n",
    "    return {\n",
    "        \"n_trials\": int(len(y_true)),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bacc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),          # sensitivity\n",
    "        \"specificity\": float(specificity),\n",
    "        \"f1\": float(f1),\n",
    "        \"auroc\": None if auroc is None else float(auroc),\n",
    "        \"auprc\": None if auprc is None else float(auprc),\n",
    "        \"brier\": None if brier is None else float(brier),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affff871-5613-409d-9063-1bf73d357862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_numpy_array(df):\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. BINARY LABEL-ENCODE GESTURE TARGET\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    # Define target gestures (BFRB-like = 1) and map others to 0\n",
    "    bfrb_gestures = [\n",
    "        \"Above ear - pull hair\",\n",
    "        \"Forehead - pull hairline\",\n",
    "        \"Forehead - scratch\",\n",
    "        \"Eyebrow - pull hair\",\n",
    "        \"Eyelash - pull hair\",\n",
    "        \"Neck - pinch skin\",\n",
    "        \"Neck - scratch\",\n",
    "        \"Cheek - pinch skin\",\n",
    "    ]\n",
    "    \n",
    "    # Assign binary labels\n",
    "    df[\"gesture\"] = df[\"gesture\"].apply(lambda g: 1 if g in bfrb_gestures else 0)\n",
    "    \n",
    "    # Save the binary class names\n",
    "    binary_classes = np.array([\"non_target\", \"target\"])\n",
    "    #np.save(\"gesture_classes_binary.npy\", binary_classes)\n",
    "    \n",
    "    # Optional: print class distribution\n",
    "    print(\"Binary label distribution:\")\n",
    "    print(df[\"gesture\"].value_counts().rename(index={0: \"non-target\", 1: \"target\"}))\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 3.  FEATURE LIST CONSTRUCTION\n",
    "    # ------------------------------------------------------------------\n",
    "    # Optionally skip thermal/TOF values → set to False to use them.\n",
    "    \n",
    "    drop_thermal_and_tof = False\n",
    "    \n",
    "    excluded_cols = {\n",
    "        \"gesture\", \"sequence_type\", \"behavior\", \"orientation\",  # train-only targets\n",
    "        \"row_id\", \"subject\", \"phase\",                            # meta\n",
    "        \"sequence_id\", \"sequence_counter\"                         # ids\n",
    "    }\n",
    "    \n",
    "    thermal_tof_cols = [c for c in df.columns if c.startswith((\"thm_\", \"tof_\"))]\n",
    "    \n",
    "    if drop_thermal_and_tof:\n",
    "        excluded_cols.update(thermal_tof_cols)\n",
    "        #print(f\"Ignoring {len(thermal_tof_cols)} thermopile/TOF channels → set drop_thermal_and_tof=False to use them.\")\n",
    "    \n",
    "    # --- NEW: demographic numeric columns --------------------------------\n",
    "    demographic_cols = [\n",
    "        \"adult_child\", \"age\", \"sex\", \"handedness\",\n",
    "        \"height_cm\", \"shoulder_to_wrist_cm\", \"elbow_to_wrist_cm\",\n",
    "    ]\n",
    "    \n",
    "    # Combine sensor + demographic feature list\n",
    "    feature_cols = [c for c in df.columns if c not in excluded_cols]\n",
    "    print(f\"Using {len(feature_cols)} feature columns for training, including demographics:\")\n",
    "    print(sorted(feature_cols)[:15], \"…\")\n",
    "    \n",
    "    # Check missing values\n",
    "    nan_total = df[feature_cols].isna().sum().sum()\n",
    "    print(f\"Total NaNs inside feature matrix: {nan_total:,}\")\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 4.  SEQUENCE BUILDING HELPERS\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    def preprocess_sequence(df_seq: pd.DataFrame, feature_columns: list[str]) -> np.ndarray:\n",
    "        \"\"\"Fill→scale a *single* sequence dataframe and return float32 numpy.\"\"\"\n",
    "        data = df_seq[feature_columns].copy()\n",
    "        data = data.ffill().bfill().fillna(0.0)\n",
    "        scaled = StandardScaler().fit_transform(data)   # per-sequence scaler (unchanged)\n",
    "        return scaled.astype(\"float32\")\n",
    "    \n",
    "    print(\"Constructing padded tensor dataset …\")\n",
    "    seq_groups = df.groupby(\"sequence_id\")\n",
    "    \n",
    "    X, seq_lengths = [], []\n",
    "    for i, (_, seq) in enumerate(seq_groups):\n",
    "        #if i and i % 500 == 0:\n",
    "        #    print(f\"  processed {i} sequences …\")\n",
    "        arr = preprocess_sequence(seq, feature_cols)\n",
    "        X.append(arr)\n",
    "        seq_lengths.append(arr.shape[0])\n",
    "    \n",
    "    pad_len = int(np.percentile(seq_lengths, 90))\n",
    "    print(f\"90th-percentile length = {pad_len} → fixed pad length chosen\")\n",
    "\n",
    "\n",
    "    \n",
    "    X = pad_sequences(X, maxlen=pad_len, dtype=\"float32\", padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "    y = seq_groups[\"gesture\"].first().values\n",
    "    num_classes = len(np.unique(y))\n",
    "    y = to_categorical(y, num_classes=num_classes)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868c3da-b616-43c2-8a13-4b7398a333b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(rows):\n",
    "        \n",
    "    results_dir = \"results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    df_subjects = pd.DataFrame(rows)\n",
    "    # Order columns nicely\n",
    "    cols = [\n",
    "        \"test_subject\", \"train_n_trials\", \"test_n_trials\",\n",
    "        \"tn\",\"fp\",\"fn\",\"tp\",\n",
    "        \"accuracy\",\"balanced_accuracy\",\"precision\",\"recall\",\"specificity\",\"f1\",\n",
    "        \"auroc\",\"auprc\",\"brier\"\n",
    "    ]\n",
    "    df_subjects = df_subjects[cols]\n",
    "    df_subjects.sort_values(\"test_subject\", inplace=True)\n",
    "    per_subject_csv = os.path.join(results_dir, f\"DetachR_num_kernels{num_kernels}_RidgeClassifierCV_loso_subject_metrics.csv\")\n",
    "    \n",
    "    \n",
    "    # save here as a file\n",
    "    df_subjects.to_csv(per_subject_csv, index=False)\n",
    "    print(f\"Per-subject metrics written to: {per_subject_csv}\")  \n",
    "    \n",
    "    # -----------------------------\n",
    "    # Aggregate across subjects\n",
    "    # -----------------------------\n",
    "    agg_metrics = (\n",
    "        df_subjects.drop(columns=[\"test_subject\"])\n",
    "                   .mean(numeric_only=True)\n",
    "                   .to_frame(\"mean\")\n",
    "    )\n",
    "    \n",
    "    agg_metrics[\"std\"] = (\n",
    "        df_subjects.drop(columns=[\"test_subject\"])\n",
    "                   .std(numeric_only=True)\n",
    "    )\n",
    "    \n",
    "    # Reset index so metrics become a column\n",
    "    df_agg = agg_metrics.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    \n",
    "    # Save\n",
    "    agg_csv = os.path.join(results_dir, f\"DetachR_num_kernels{num_kernels}_RidgeClassifierCV_loso_agg_metrics.csv\")\n",
    "    df_agg.to_csv(agg_csv, index=False)\n",
    "    print(f\"Aggregated metrics written to: {agg_csv}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c5467-ade7-4874-901d-cbf8ced5837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Imports loaded\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  LOAD TRAIN SENSOR DATA + DEMOGRAPHICS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Loading sensor dataset …\")\n",
    "root = '/Users/ashhadulislam/projects/general_data/CMI/ Detect Behavior with Sensor Data/cmi-detect-behavior-with-sensor-data/'\n",
    "\n",
    "df = pd.read_csv(f\"{root}/train.csv\")\n",
    "print(f\"Loaded {len(df):,} rows of sensor frames\")\n",
    "\n",
    "# --- NEW: merge participant demographics on the key `subject` --------\n",
    "print(\"Merging demographic attributes …\")\n",
    "demographics = pd.read_csv(f\"{root}/train_demographics.csv\")\n",
    "df = df.merge(demographics, on=\"subject\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001528d5-b2fd-45c9-aa45-c3c785edc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b356d-b7d1-40a3-9497-fc8b35fd5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique subjects\n",
    "subjects = df['subject'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce362c-ef40-49d1-a822-9671641d70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []                 # per-test-subject metric rows\n",
    "counter=0\n",
    "for test_subject in subjects:\n",
    "    start=time.time()\n",
    "    print(f'Test subject: {test_subject}')\n",
    "    train_df=df[df['subject']!=test_subject]\n",
    "    test_df=df[df['subject']==test_subject]\n",
    "    print(train_df.shape,test_df.shape)\n",
    "    #break\n",
    "    X_test,y_test=get_numpy_array(test_df)\n",
    "    X_train,y_train=get_numpy_array(train_df)\n",
    "    print('Obtained tensors for train and test')\n",
    "    if len(Counter(test_df['gesture']).keys())==1:\n",
    "        print(test_subject, df.shape[0],train_df.shape[0],test_df.shape[0])    \n",
    "        print(Counter(train_df['gesture']), len(Counter(test_df['gesture']).keys()))\n",
    "    print(X_test.shape,y_test.shape)\n",
    "    print(X_test.shape,y_test.shape,X_train.shape,y_train.shape)\n",
    "    \n",
    "\n",
    "    # 1) Transpose to (n_samples, n_channels, n_timepoints)\n",
    "    X_train = np.transpose(X_train, (0, 2, 1))\n",
    "    X_test  = np.transpose(X_test,  (0, 2, 1))\n",
    "    \n",
    "    # 2a) Option A: pad/crop so all time lengths match (choose a common T)\n",
    "    T = min(X_train.shape[-1], X_test.shape[-1])  # or max(...) and pad\n",
    "    X_train = X_train[..., :T]   # or pad to T\n",
    "    X_test  = X_test[..., :T]\n",
    "    \n",
    "    # 3) Make y 1-D labels\n",
    "    # If y is one-hot, convert with argmax; if already class ids, ravel.\n",
    "    y_train_1d = y_train.argmax(axis=1) if y_train.ndim == 2 else y_train.ravel()\n",
    "    y_test_1d  = y_test.argmax(axis=1)  if y_test.ndim == 2 else y_test.ravel()\n",
    "\n",
    "    \n",
    "\n",
    "    num_models = 5\n",
    "    num_kernels = 100\n",
    "    print('going to train on detach')\n",
    "    clf = DetachEnsemble(num_models=num_models, num_kernels=num_kernels)\n",
    "    clf.fit(X_train, y_train_1d)        \n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    \n",
    "    m=compute_binary_metrics(y_test_1d, y_test_pred, None)\n",
    "    m.update({            \n",
    "            \"test_subject\": test_subject,\n",
    "            \"train_n_trials\": int(len(y_train)),\n",
    "            \"test_n_trials\": int(len(y_test)),\n",
    "        })\n",
    "    print(m)\n",
    "    rows.append(m)\n",
    "    \n",
    "    \n",
    "    end=time.time()\n",
    "    save_results(rows)\n",
    "    print(end-start)\n",
    "    counter+=1    \n",
    "    if counter>3:\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fab68-b6ad-4ac3-8d05-dbec5965f503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310detachKern",
   "language": "python",
   "name": "py310detachkern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
